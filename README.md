# Word_Embedding_Layer_NLP:
This ipynb file is about adding an embedding a layer to train the model on any given dataset and then convert words into vectors. Here we have first applied one hot encoding to get indexes assigned to each unique word and then use embedding layer to convert them into vectors with some feature representation.
# Word2Vec vs Embedding Layer:
Word2Vec is a pre trained model which has been trained on a large corpus therefore that can be used to convert words into vectors if your dataset is not large enough. However, Embedding layer allows you to train your model on your dataset in order to convert them into vectors. this is good when your language differs from the pre-trained model language.
